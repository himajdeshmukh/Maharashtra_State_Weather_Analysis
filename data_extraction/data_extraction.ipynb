"""
Script: fetch_nasa_power_maharashtra.py

Purpose:
    District-wise extraction of daily weather data for Maharashtra
    using NASA POWER AG datasets with region classification and
    large-dataset handling.

Key Features:
    - 40+ years daily data support
    - District and region tagging
    - Excel, CSV, and Parquet outputs
    - Config-driven parameter selection

Notes:
    - No analysis or visualization is performed here
    - Designed for reproducible and scalable data extraction
"""
import requests
import pandas as pd
import yaml
import time
from tqdm import tqdm
import os
import math

# Maharashtra districts with coordinates (latitude, longitude) covering all corners
MAHARASHTRA_DISTRICTS = {
    # East Vidarbha (Eastern border)
    "Gadchiroli": (19.6660, 80.3330),  # Easternmost
    "Chandrapur": (19.9615, 79.2961),  # East Central
    "Bhandara": (21.1667, 79.6500),    # Northeast
    "Gondia": (21.4500, 80.2000),      # Northeastern corner
    "Nagpur": (21.1458, 79.0882),      # Central East Vidarbha
    "Wardha": (20.7453, 78.6022),      # West East Vidarbha
    
    # West Vidarbha (Western border)
    "Amravati": (20.9374, 77.7795),    # West Central Vidarbha
    "Yavatmal": (20.3888, 78.1204),    # Southwest Vidarbha
    "Akola": (20.7000, 77.0000),       # Western border
    "Buldhana": (20.5333, 76.1833),    # Northwest border West Vidarbha
    "Washim": (20.1000, 77.1500),      # North Central West Vidarbha
    
    # Marathwada Region (Central-East)
    "Aurangabad": (19.8762, 75.3433),  # Central Marathwada
    "Jalna": (19.8342, 75.8804),       # East Marathwada
    "Nanded": (19.1383, 77.3210),      # Southeast border
    "Hingoli": (19.7167, 77.1500),     # East Central
    "Parbhani": (19.2667, 76.7833),    # Central Marathwada
    "Latur": (18.4088, 76.5604),       # Southern Marathwada
    "Beed": (18.9894, 75.7560),        # Southwest Marathwada
    
    # Western Maharashtra (West border)
    "Pune": (18.5204, 73.8567),        # West Central
    "Satara": (17.6805, 74.0183),      # Southwest
    "Kolhapur": (16.7050, 74.2433),    # Southernmost West
    "Sangli": (16.8667, 74.5667),      # Southern West
    "Solapur": (17.6599, 75.9064),     # Southeast West
    "Ahmednagar": (19.0952, 74.7496),  # North West
    
    # North Maharashtra (North border)
    "Nashik": (20.0110, 73.7902),      # Northwest
    "Dhule": (20.9020, 74.7774),       # North Central
    "Jalgaon": (21.0486, 75.7981),     # Northeast West
    "Nandurbar": (21.3667, 74.2667),   # Northernmost
    
    # Konkan Region (Coastal - West border)
    "Mumbai City": (19.0760, 72.8777), # Westernmost
    "Mumbai Suburban": (19.2167, 72.8333), # West Coastal
    "Thane": (19.2183, 72.9781),       # West Coastal North
    "Raigad": (18.7500, 73.4167),      # West Central Coastal
    "Ratnagiri": (17.0000, 73.3000),   # Southwest Coastal
    "Sindhudurg": (16.2000, 73.6833),  # Southernmost Coastal
}

# List of parameters known to be supported for daily AG data
SUPPORTED_PARAMETERS = {
    "T2M_MAX", "T2M_MIN", "T2M", "PRECTOTCORR", "RH2M",
    "ALLSKY_SFC_SW_DWN", "WS10M", "GWETTOP", "GWETROOT",
    "CLOUD_AMT", "ALLSKY_SFC_PAR_TOT", "TS"
}

def calculate_dataset_size(start_date, end_date, num_parameters, num_districts):
    """Calculate approximate dataset size"""
    start_dt = pd.to_datetime(start_date, format='%Y%m%d')
    end_dt = pd.to_datetime(end_date, format='%Y%m%d')
    days = (end_dt - start_dt).days + 1
    
    total_records = days * num_districts
    excel_limit = 1048576  # Excel row limit
    
    print(f"\nğŸ“Š Dataset Size Estimation:")
    print(f"   ğŸ“… Time period: {days} days")
    print(f"   ğŸ—ºï¸  Districts: {num_districts}")
    print(f"   ğŸ“ˆ Parameters: {num_parameters}")
    print(f"   ğŸ“‹ Total records: {total_records:,}")
    print(f"   ğŸ’¾ Excel row limit: {excel_limit:,}")
    
    if total_records > excel_limit:
        print(f"   âš ï¸  WARNING: Dataset exceeds Excel row limit!")
        return False, total_records
    else:
        print(f"   âœ… Dataset within Excel limits")
        return True, total_records

def fetch_nasa_power_data(lat, lon, start_date, end_date, parameters, district_name):
    """
    Fetch daily NASA POWER data for a single district.
    """
    # Filter only supported parameters
    filtered_params = [p for p in parameters if p in SUPPORTED_PARAMETERS]
    
    if not filtered_params:
        print(f"âš ï¸ No valid parameters for {district_name}. Skipping.")
        return None

    base_url = "https://power.larc.nasa.gov/api/temporal/daily/point"
    params = {
        "start": start_date,
        "end": end_date,
        "latitude": lat,
        "longitude": lon,
        "parameters": ",".join(filtered_params),
        "format": "JSON",
        "community": "AG"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=30)
        
        if response.status_code != 200:
            print(f"âŒ Error fetching data for {district_name}: HTTP {response.status_code}")
            return None
        
        data = response.json()
        daily_data = data['properties']['parameter']
        
        df = pd.DataFrame(daily_data)
        df.index = pd.to_datetime(df.index, format="%Y%m%d")
        df['District'] = district_name
        df['Region'] = get_region(district_name)
        
        return df
        
    except requests.exceptions.Timeout:
        print(f"âŒ Timeout fetching data for {district_name}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ Request error for {district_name}: {e}")
        return None
    except KeyError:
        print(f"âŒ Unexpected API response format for {district_name}")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error for {district_name}: {e}")
        return None

def get_region(district_name):
    """Map district to detailed region - CORRECTED REGIONAL CLASSIFICATION"""
    regions = {
        # East Vidarbha
        "Gadchiroli": "East Vidarbha", 
        "Chandrapur": "East Vidarbha", 
        "Bhandara": "East Vidarbha", 
        "Gondia": "East Vidarbha",
        "Nagpur": "East Vidarbha",      # MOVED to East Vidarbha
        "Wardha": "East Vidarbha",      # MOVED to East Vidarbha
        
        # West Vidarbha
        "Amravati": "West Vidarbha", 
        "Yavatmal": "West Vidarbha", 
        "Akola": "West Vidarbha",
        "Buldhana": "West Vidarbha",    # MOVED to West Vidarbha
        "Washim": "West Vidarbha",      # MOVED to West Vidarbha
        
        # Marathwada Region
        "Aurangabad": "Marathwada",     # MOVED to Marathwada
        "Jalna": "Marathwada",          # MOVED to Marathwada
        "Nanded": "Marathwada", 
        "Hingoli": "Marathwada", 
        "Parbhani": "Marathwada", 
        "Latur": "Marathwada", 
        "Beed": "Marathwada",
        
        # Western Maharashtra
        "Pune": "Western Maharashtra", 
        "Satara": "Western Maharashtra",
        "Kolhapur": "Western Maharashtra", 
        "Sangli": "Western Maharashtra",
        "Solapur": "Western Maharashtra", 
        "Ahmednagar": "Western Maharashtra",
        
        # North Maharashtra
        "Nashik": "North Maharashtra", 
        "Dhule": "North Maharashtra",
        "Jalgaon": "North Maharashtra", 
        "Nandurbar": "North Maharashtra",
        
        # Konkan Region
        "Mumbai City": "Konkan", 
        "Mumbai Suburban": "Konkan",
        "Thane": "Konkan", 
        "Raigad": "Konkan", 
        "Ratnagiri": "Konkan", 
        "Sindhudurg": "Konkan"
    }
    return regions.get(district_name, "Unknown")

def get_geographical_coverage():
    """Calculate and display geographical coverage"""
    lats = [coords[0] for coords in MAHARASHTRA_DISTRICTS.values()]
    lons = [coords[1] for coords in MAHARASHTRA_DISTRICTS.values()]
    
    coverage = {
        "Northernmost": max(lats),
        "Southernmost": min(lats),
        "Easternmost": max(lons),
        "Westernmost": min(lons),
        "Total Districts": len(MAHARASHTRA_DISTRICTS),
        "Regions Covered": len(set(get_region(district) for district in MAHARASHTRA_DISTRICTS.keys()))
    }
    return coverage

def save_by_year(combined_df, output_file):
    """Save data by year to avoid Excel row limits"""
    combined_df['Year'] = combined_df['Date'].dt.year
    
    for year in combined_df['Year'].unique():
        year_df = combined_df[combined_df['Year'] == year].copy()
        year_df.drop('Year', axis=1, inplace=True)
        
        year_filename = f"maharashtra_data/{output_file.replace('.xlsx', '')}_{year}.xlsx"
        
        with pd.ExcelWriter(year_filename, engine='openpyxl') as writer:
            year_df.to_excel(writer, sheet_name=f'Data_{year}', index=False)
            
            # Add summary sheet
            summary = year_df.groupby(['Region', 'District']).size().reset_index(name='Record_Count')
            summary.to_excel(writer, sheet_name='Summary', index=False)
        
        print(f"ğŸ’¾ Saved {year} data: {len(year_df):,} records")

def save_data_optimized(all_data, output_file, start_date, end_date):
    """Save data with optimization for large datasets"""
    
    if not all_data:
        print("âŒ No data to save.")
        return
    
    # Combine all data
    print("\nğŸ“Š Combining data from all districts...")
    combined_df = pd.concat(all_data, ignore_index=False)
    
    # Reset index to make Date a column
    combined_df.reset_index(inplace=True)
    combined_df.rename(columns={'index': 'Date'}, inplace=True)
    
    # Reorder columns
    column_order = ['Date', 'District', 'Region'] + [col for col in combined_df.columns 
                                                   if col not in ['Date', 'District', 'Region']]
    combined_df = combined_df[column_order]
    
    # Sort by Date and District
    combined_df.sort_values(['Date', 'District'], inplace=True)
    
    # Create results directory if it doesn't exist
    os.makedirs("maharashtra_data", exist_ok=True)
    
    # Calculate dataset size
    total_records = len(combined_df)
    excel_limit = 1048576
    
    if total_records > excel_limit:
        print(f"âš ï¸ Large dataset detected: {total_records:,} records")
        print("ğŸ’¡ Implementing optimized storage strategy...")
        
        # Strategy 1: Save as multiple Excel files by year
        save_by_year(combined_df, output_file)
        
        # Strategy 2: Save as CSV (no row limits)
        csv_path = f"maharashtra_data/{output_file.replace('.xlsx', '.csv')}"
        print(f"ğŸ’¾ Saving as CSV: {csv_path}")
        combined_df.to_csv(csv_path, index=False)
        
        # Strategy 3: Save as Parquet for efficient storage
        parquet_path = f"maharashtra_data/{output_file.replace('.xlsx', '.parquet')}"
        print(f"ğŸ’¾ Saving as Parquet: {parquet_path}")
        combined_df.to_parquet(parquet_path, index=False)
        
        print(f"âœ… Data saved in multiple formats to handle large size")
        
    else:
        # Save to single Excel file
        output_path = f"maharashtra_data/{output_file}"
        if not output_path.endswith(".xlsx"):
            output_path += ".xlsx"
            
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Save main data
            combined_df.to_excel(writer, sheet_name='All_Districts', index=False)
            
            # Save summary by region
            summary = combined_df.groupby(['Region', 'District']).size().reset_index(name='Record_Count')
            summary.to_excel(writer, sheet_name='Summary', index=False)
            
            # Save coordinates information
            coords_data = []
            for district, (lat, lon) in MAHARASHTRA_DISTRICTS.items():
                coords_data.append({
                    'District': district,
                    'Region': get_region(district),
                    'Latitude': lat,
                    'Longitude': lon
                })
            coords_df = pd.DataFrame(coords_data)
            coords_df.to_excel(writer, sheet_name='Coordinates', index=False)
        
        print(f"ğŸ’¾ Data saved to: {output_path}")

def fetch_all_districts_data(start_date, end_date, parameters, output_file):
    """
    Fetch data for all Maharashtra districts and save to a single Excel file.
    """
    all_data = []
    failed_districts = []
    
    print(f"ğŸš€ Starting data fetch for {len(MAHARASHTRA_DISTRICTS)} districts...")
    print(f"ğŸ“… Period: {start_date} to {end_date}")
    print(f"ğŸ“Š Parameters: {', '.join(parameters)}")
    
    # Check dataset size first
    within_limits, total_estimated = calculate_dataset_size(
        start_date, end_date, len(parameters), len(MAHARASHTRA_DISTRICTS)
    )
    
    if not within_limits:
        print("ğŸ’¡ Consider reducing time range or number of districts")
        proceed = input("Continue anyway? (y/n): ").strip().lower()
        if proceed != 'y':
            print("Data fetch cancelled.")
            return
    
    # Display geographical coverage
    coverage = get_geographical_coverage()
    print("\nğŸ—ºï¸  Geographical Coverage:")
    print(f"   ğŸ“ Northernmost: {coverage['Northernmost']:.4f}Â°N")
    print(f"   ğŸ“ Southernmost: {coverage['Southernmost']:.4f}Â°N")
    print(f"   ğŸ“ Easternmost: {coverage['Easternmost']:.4f}Â°E")
    print(f"   ğŸ“ Westernmost: {coverage['Westernmost']:.4f}Â°E")
    print("-" * 60)
    
    for district, (lat, lon) in tqdm(MAHARASHTRA_DISTRICTS.items(), desc="Fetching district data"):
        print(f"\nğŸ“ Processing {district} ({lat:.4f}Â°N, {lon:.4f}Â°E) - {get_region(district)}...")
        
        df = fetch_nasa_power_data(lat, lon, start_date, end_date, parameters, district)
        
        if df is not None:
            all_data.append(df)
            print(f"âœ… Successfully fetched data for {district}")
        else:
            failed_districts.append(district)
            print(f"âŒ Failed to fetch data for {district}")
        
        # Add delay to be respectful to the API
        time.sleep(1)
    
    if not all_data:
        print("âŒ No data was successfully fetched. Exiting.")
        return
    
    # Save data with optimization
    save_data_optimized(all_data, output_file, start_date, end_date)
    
    print(f"\nğŸ‰ Data fetch completed!")
    print(f"âœ… Successfully fetched: {len(all_data)} districts")
    print(f"âŒ Failed: {len(failed_districts)} districts")
    
    if failed_districts:
        print(f"Failed districts: {', '.join(failed_districts)}")
    
    # Print regional summary
    print("\nğŸ“ˆ Regional Summary (Corrected Classification):")
    regions_summary = {}
    for district in MAHARASHTRA_DISTRICTS.keys():
        region = get_region(district)
        if region not in regions_summary:
            regions_summary[region] = []
        regions_summary[region].append(district)
    
    for region, districts in regions_summary.items():
        print(f"   {region}: {len(districts)} districts")
        print(f"     ğŸ“ {', '.join(districts)}")

def main():
    """Main function to run the data fetch for all districts"""
    
    # Load YAML config
    try:
        with open("config.yaml", "r") as file:
            config = yaml.safe_load(file)
    except FileNotFoundError:
        print("âŒ config.yaml file not found. Using default parameters.")
        config = {
            "parameters": {
                "T2M_MAX": True,
                "T2M_MIN": True,
                "T2M": True,
                "PRECTOTCORR": True,
                "RH2M": True
            }
        }
    
    # Extract parameters set to True
    parameters_dict = config.get("parameters", {})
    selected_parameters = [param for param, enabled in parameters_dict.items() if enabled]
    
    # Filter only supported parameters
    selected_parameters = [p for p in selected_parameters if p in SUPPORTED_PARAMETERS]
    
    if not selected_parameters:
        print("âŒ No valid parameters selected. Please check config.yaml")
        return
    
    print("ğŸ”ï¸ Maharashtra District Weather Data Fetcher")
    print("=" * 60)
    print("ğŸ—ºï¸  Comprehensive Coverage - All Corners of Maharashtra")
    print("=" * 60)
    
    # Display district distribution by region with CORRECTED classification
    print("Districts distribution by region (Corrected):")
    regions_summary = {}
    for district in MAHARASHTRA_DISTRICTS.keys():
        region = get_region(district)
        if region not in regions_summary:
            regions_summary[region] = []
        regions_summary[region].append(district)
    
    for region, districts in regions_summary.items():
        print(f"  {region}: {len(districts)} districts")
        print(f"    ğŸ“ {', '.join(districts)}")
    
    print("-" * 60)
    
    # User input
    start_date = input("Enter start date (YYYYMMDD, e.g., 19850101): ").strip()
    end_date = input("Enter end date (YYYYMMDD, e.g., 20241231): ").strip()
    output_file = input("Enter output filename (e.g., 'maharashtra_weather_data.xlsx'): ").strip()
    
    if not output_file:
        output_file = f"maharashtra_weather_{start_date}_{end_date}.xlsx"
    
    # Confirm before proceeding
    print(f"\nğŸ“‹ Summary:")
    print(f"   Period: {start_date} to {end_date}")
    print(f"   Total Districts: {len(MAHARASHTRA_DISTRICTS)}")
    print(f"   Regions Covered: {len(regions_summary)}")
    print(f"   Parameters: {', '.join(selected_parameters)}")
    print(f"   Output file: {output_file}")
    
    confirm = input("\nProceed with data fetch? (y/n): ").strip().lower()
    if confirm != 'y':
        print("Data fetch cancelled.")
        return
    
    # Fetch data
    fetch_all_districts_data(start_date, end_date, selected_parameters, output_file)

if __name__ == "__main__":
    main()
